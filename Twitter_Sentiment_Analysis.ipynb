{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monica Parihar\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import jsonpickle\n",
    "import csv\n",
    "import time\n",
    "st = time.time()\n",
    "API_KEY=\"z5fwBpqXKReGZdR9qI2VyYsvs\"\n",
    "API_SECRET=\"5GaSPBTvEyKvv4G2rku1d13Lw05iz6MbnwKCprfjatYClkmsBq\"\n",
    "ACCESS_TOKEN=\"1479289146-kvsxXbGz5mWKmunsKpeoeJwOqwZ7VZUdeNXggx4\"\n",
    "ACCESS_TOKEN_SECRET=\"kAk1y1Jk75FFU8jnILW57qXoiRT4bTTFikbTYEZBMDdob\"\n",
    "auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "print(api.me().name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsPerQuery = 100#this is the maximum provided by API\n",
    "max_tweets = 100 # just for the sake of While loop\n",
    "fName = 'sharomo.txt' # where i save the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the tweeets..takes some time..\n"
     ]
    }
   ],
   "source": [
    "since_id = None\n",
    "max_id = -1\n",
    "tweet_count = 0\n",
    "print(\"Downloading the tweeets..takes some time..\")\n",
    "\n",
    "search_query=\"\\\"#SwachhBharat\\\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading hashtag\"#SwachhBharat\"\n",
      "Successfully downloaded 1 tweets\n",
      "Successfully downloaded 2 tweets\n",
      "Successfully downloaded 3 tweets\n",
      "Successfully downloaded 4 tweets\n",
      "Successfully downloaded 5 tweets\n",
      "Successfully downloaded 6 tweets\n",
      "Successfully downloaded 7 tweets\n",
      "Successfully downloaded 8 tweets\n",
      "Successfully downloaded 9 tweets\n",
      "Successfully downloaded 10 tweets\n",
      "Successfully downloaded 11 tweets\n",
      "Successfully downloaded 12 tweets\n",
      "Successfully downloaded 13 tweets\n",
      "Successfully downloaded 14 tweets\n",
      "Successfully downloaded 15 tweets\n",
      "Successfully downloaded 16 tweets\n",
      "Successfully downloaded 17 tweets\n",
      "Successfully downloaded 18 tweets\n",
      "Successfully downloaded 19 tweets\n",
      "Successfully downloaded 20 tweets\n",
      "Successfully downloaded 21 tweets\n",
      "Successfully downloaded 22 tweets\n",
      "Successfully downloaded 23 tweets\n",
      "Successfully downloaded 24 tweets\n",
      "Successfully downloaded 25 tweets\n",
      "Successfully downloaded 26 tweets\n",
      "Successfully downloaded 27 tweets\n",
      "Successfully downloaded 28 tweets\n",
      "Successfully downloaded 29 tweets\n",
      "Successfully downloaded 30 tweets\n",
      "Successfully downloaded 31 tweets\n",
      "Successfully downloaded 32 tweets\n",
      "Successfully downloaded 33 tweets\n",
      "Successfully downloaded 34 tweets\n",
      "Successfully downloaded 35 tweets\n",
      "Successfully downloaded 36 tweets\n",
      "Successfully downloaded 37 tweets\n",
      "Successfully downloaded 38 tweets\n",
      "Successfully downloaded 39 tweets\n",
      "Successfully downloaded 40 tweets\n",
      "Successfully downloaded 41 tweets\n",
      "Successfully downloaded 42 tweets\n",
      "Successfully downloaded 43 tweets\n",
      "Successfully downloaded 44 tweets\n",
      "Successfully downloaded 45 tweets\n",
      "Successfully downloaded 46 tweets\n",
      "Successfully downloaded 47 tweets\n",
      "Successfully downloaded 48 tweets\n",
      "Successfully downloaded 49 tweets\n",
      "Successfully downloaded 50 tweets\n",
      "Successfully downloaded 51 tweets\n",
      "Successfully downloaded 52 tweets\n",
      "Successfully downloaded 53 tweets\n",
      "Successfully downloaded 54 tweets\n",
      "Successfully downloaded 55 tweets\n",
      "Successfully downloaded 56 tweets\n",
      "Successfully downloaded 57 tweets\n",
      "Successfully downloaded 58 tweets\n",
      "Successfully downloaded 59 tweets\n",
      "Successfully downloaded 60 tweets\n",
      "Successfully downloaded 61 tweets\n",
      "Successfully downloaded 62 tweets\n",
      "Successfully downloaded 63 tweets\n",
      "Successfully downloaded 64 tweets\n",
      "Successfully downloaded 65 tweets\n",
      "Successfully downloaded 66 tweets\n",
      "Successfully downloaded 67 tweets\n",
      "Successfully downloaded 68 tweets\n",
      "Successfully downloaded 69 tweets\n",
      "Successfully downloaded 70 tweets\n",
      "Successfully downloaded 71 tweets\n",
      "Successfully downloaded 72 tweets\n",
      "Successfully downloaded 73 tweets\n",
      "Successfully downloaded 74 tweets\n",
      "Successfully downloaded 75 tweets\n",
      "Successfully downloaded 76 tweets\n",
      "Successfully downloaded 77 tweets\n",
      "Successfully downloaded 78 tweets\n",
      "Successfully downloaded 79 tweets\n",
      "Successfully downloaded 80 tweets\n",
      "Successfully downloaded 81 tweets\n",
      "Successfully downloaded 82 tweets\n",
      "Successfully downloaded 83 tweets\n",
      "Successfully downloaded 84 tweets\n",
      "Successfully downloaded 85 tweets\n",
      "Successfully downloaded 86 tweets\n",
      "Successfully downloaded 87 tweets\n",
      "Successfully downloaded 88 tweets\n",
      "Successfully downloaded 89 tweets\n",
      "Successfully downloaded 90 tweets\n",
      "Successfully downloaded 91 tweets\n",
      "Successfully downloaded 92 tweets\n",
      "Successfully downloaded 93 tweets\n",
      "Successfully downloaded 94 tweets\n",
      "Successfully downloaded 95 tweets\n",
      "Successfully downloaded 96 tweets\n",
      "Successfully downloaded 97 tweets\n",
      "Successfully downloaded 98 tweets\n",
      "Successfully downloaded 99 tweets\n",
      "Successfully downloaded 100 tweets\n",
      "A total of 100 tweets are downloaded and saved to sharomo.txt\n",
      "Total time taken is  14.756608009338379 seconds.\n"
     ]
    }
   ],
   "source": [
    "search_query=\"\\\"#SwachhBharat\\\"\"\n",
    "x=0\n",
    "with open(fName,'w') as f:\n",
    "    print(\"Downloading hashtag\" + search_query)\n",
    "    \n",
    "    while(tweet_count<max_tweets):\n",
    "        try:\n",
    "            if(max_id<=0):\n",
    "                if(not since_id):\n",
    "                    new_tweets = api.search(q=search_query,count=tweetsPerQuery,lang=\"en\",tweet_mode='extended')\n",
    "                else:\n",
    "                    new_tweets = api.search(q=search_query,count=tweetsPerQuery,lang=\"en\",tweet_mode='extended',since_id=since_id)\n",
    "            \n",
    "            else:\n",
    "                if(not since_id):\n",
    "                    new_tweets = api.search(q=search_query,count=tweetsPerQuery,lang=\"en\",tweet_mode='extended',max_id=str(max_id-1))\n",
    "                else:\n",
    "                    new_tweets = api.search(q=search_query,count=tweetsPerQuery,lang=\"en\",tweet_mode='extended',max_id=str(max_id-1),since_id=since_id)\n",
    "            \n",
    "            # Tweets Exhausted\n",
    "            if(not new_tweets):\n",
    "                print(\"No more tweets found!!\")\n",
    "                break\n",
    "            # write all the new_tweets to a json file\n",
    "            for tweet in new_tweets:\n",
    "                f.write(jsonpickle.encode(tweet._json,unpicklable=False)+'\\n')\n",
    "                tweet_count+=1\n",
    "                print(\"Successfully downloaded {0} tweets\".format(tweet_count))\n",
    "                max_id=new_tweets[-1].id\n",
    "        # in case of any error\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Some error!!:\"+str(e))\n",
    "            break\n",
    "end = time.time()\n",
    "print(\"A total of {0} tweets are downloaded and saved to {1}\".format(tweet_count,fName))\n",
    "print(\"Total time taken is \",end-st,\"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI 100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "f = open('som2.csv','a',encoding='utf-8')\n",
    "csvWriter = csv.writer(f)\n",
    "headers=['full_text','retweet_count','user_followers_count','favorite_count','place','coordinates','geo','created_at','id_str']\n",
    "csvWriter.writerow(headers)\n",
    "for inputFile in ['sharomo.txt']:#all the text-file names you want to convert to Csv in the sae folder as this code\n",
    "    tweets = []\n",
    "    for line in open(inputFile, 'r'):\n",
    "        tweets.append(json.loads(line))\n",
    "\n",
    "    print('HI',len(tweets))\n",
    "    \n",
    "    count_lines=0\n",
    "\n",
    "    for tweet in tweets:\n",
    "        try:\n",
    "            csvWriter.writerow([tweet['full_text'],tweet['retweet_count'],tweet['user']['followers_count'],tweet['favorite_count'],tweet['place'],tweet['coordinates'],tweet['geo'],tweet['created_at'],str(tweet['id_str'])])\n",
    "            count_lines+=1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    print(count_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 75 entries, 0 to 74\n",
      "Data columns (total 9 columns):\n",
      "full_text               75 non-null object\n",
      "retweet_count           75 non-null int64\n",
      "user_followers_count    75 non-null int64\n",
      "favorite_count          75 non-null int64\n",
      "place                   0 non-null float64\n",
      "coordinates             0 non-null float64\n",
      "geo                     0 non-null float64\n",
      "created_at              75 non-null object\n",
      "id_str                  75 non-null int64\n",
      "dtypes: float64(3), int64(4), object(2)\n",
      "memory usage: 5.4+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('som2.csv', encoding = 'unicode_escape')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>place</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>geo</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RT @JayasreeVijayan: This is  VARANASI guys !!...</td>\n",
       "      <td>439</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Feb 08 06:52:10 +0000 2020</td>\n",
       "      <td>1226035980026535937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@noida_authority @CeoNoida Please Do permanent...</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Feb 08 06:51:51 +0000 2020</td>\n",
       "      <td>1226035898854199296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>RT @JayasreeVijayan: This is  VARANASI guys !!...</td>\n",
       "      <td>439</td>\n",
       "      <td>543</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Feb 08 06:51:36 +0000 2020</td>\n",
       "      <td>1226035836258410496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>RT @STPIPune: @rsprasad @SanjayDhotreMP @Secre...</td>\n",
       "      <td>57</td>\n",
       "      <td>753</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Feb 08 06:51:02 +0000 2020</td>\n",
       "      <td>1226035693295525888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>RT @STPIPune: @rsprasad @SanjayDhotreMP @Secre...</td>\n",
       "      <td>83</td>\n",
       "      <td>753</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Feb 08 06:50:52 +0000 2020</td>\n",
       "      <td>1226035650979221505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>RT @STPIBengaluru: #Shramadhan by STPI-Mysuru ...</td>\n",
       "      <td>109</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Feb 08 06:26:06 +0000 2020</td>\n",
       "      <td>1226029419367501824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>RT @smpurban: Unsung heroes of Singrauli\\r\\nWh...</td>\n",
       "      <td>57</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Feb 08 06:25:58 +0000 2020</td>\n",
       "      <td>1226029383636226050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>RT @stpitvpm: Workshop on 'Waste management, P...</td>\n",
       "      <td>52</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Feb 08 06:25:50 +0000 2020</td>\n",
       "      <td>1226029351264649217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>RT @JayasreeVijayan: This is  VARANASI guys !!...</td>\n",
       "      <td>439</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Feb 08 06:25:21 +0000 2020</td>\n",
       "      <td>1226029228199538688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>RT @JayasreeVijayan: This is  VARANASI guys !!...</td>\n",
       "      <td>439</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Feb 08 06:25:17 +0000 2020</td>\n",
       "      <td>1226029213293006848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            full_text  retweet_count  \\\n",
       "0   RT @JayasreeVijayan: This is  VARANASI guys !!...            439   \n",
       "1   @noida_authority @CeoNoida Please Do permanent...              0   \n",
       "2   RT @JayasreeVijayan: This is  VARANASI guys !!...            439   \n",
       "3   RT @STPIPune: @rsprasad @SanjayDhotreMP @Secre...             57   \n",
       "4   RT @STPIPune: @rsprasad @SanjayDhotreMP @Secre...             83   \n",
       "..                                                ...            ...   \n",
       "70  RT @STPIBengaluru: #Shramadhan by STPI-Mysuru ...            109   \n",
       "71  RT @smpurban: Unsung heroes of Singrauli\\r\\nWh...             57   \n",
       "72  RT @stpitvpm: Workshop on 'Waste management, P...             52   \n",
       "73  RT @JayasreeVijayan: This is  VARANASI guys !!...            439   \n",
       "74  RT @JayasreeVijayan: This is  VARANASI guys !!...            439   \n",
       "\n",
       "    user_followers_count  favorite_count  place  coordinates  geo  \\\n",
       "0                    290               0    NaN          NaN  NaN   \n",
       "1                     28               0    NaN          NaN  NaN   \n",
       "2                    543               0    NaN          NaN  NaN   \n",
       "3                    753               0    NaN          NaN  NaN   \n",
       "4                    753               0    NaN          NaN  NaN   \n",
       "..                   ...             ...    ...          ...  ...   \n",
       "70                    34               0    NaN          NaN  NaN   \n",
       "71                    65               0    NaN          NaN  NaN   \n",
       "72                    34               0    NaN          NaN  NaN   \n",
       "73                    54               0    NaN          NaN  NaN   \n",
       "74                    21               0    NaN          NaN  NaN   \n",
       "\n",
       "                        created_at               id_str  \n",
       "0   Sat Feb 08 06:52:10 +0000 2020  1226035980026535937  \n",
       "1   Sat Feb 08 06:51:51 +0000 2020  1226035898854199296  \n",
       "2   Sat Feb 08 06:51:36 +0000 2020  1226035836258410496  \n",
       "3   Sat Feb 08 06:51:02 +0000 2020  1226035693295525888  \n",
       "4   Sat Feb 08 06:50:52 +0000 2020  1226035650979221505  \n",
       "..                             ...                  ...  \n",
       "70  Sat Feb 08 06:26:06 +0000 2020  1226029419367501824  \n",
       "71  Sat Feb 08 06:25:58 +0000 2020  1226029383636226050  \n",
       "72  Sat Feb 08 06:25:50 +0000 2020  1226029351264649217  \n",
       "73  Sat Feb 08 06:25:21 +0000 2020  1226029228199538688  \n",
       "74  Sat Feb 08 06:25:17 +0000 2020  1226029213293006848  \n",
       "\n",
       "[75 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "0\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(df.index))#14195\n",
    "serlis=df.duplicated().tolist()\n",
    "print(serlis.count(True))#112\n",
    "serlis=df.duplicated(['full_text']).tolist()\n",
    "print(serlis.count(True))#8585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>place</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>geo</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RT @JayasreeVijayan: This is  VARANASI guys !!...</td>\n",
       "      <td>439</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Feb 08 06:52:10 +0000 2020</td>\n",
       "      <td>1226035980026535937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@noida_authority @CeoNoida Please Do permanent...</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Feb 08 06:51:51 +0000 2020</td>\n",
       "      <td>1226035898854199296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>RT @STPIPune: @rsprasad @SanjayDhotreMP @Secre...</td>\n",
       "      <td>57</td>\n",
       "      <td>753</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Feb 08 06:51:02 +0000 2020</td>\n",
       "      <td>1226035693295525888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>RT @_WeMeanToClean: Please retweet: Let's #Sto...</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Feb 08 06:50:33 +0000 2020</td>\n",
       "      <td>1226035572478750721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>RT @chennaistpi: #SwachhtaPakhwada Pledge take...</td>\n",
       "      <td>136</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Feb 08 06:50:25 +0000 2020</td>\n",
       "      <td>1226035539423326208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text  retweet_count  \\\n",
       "0  RT @JayasreeVijayan: This is  VARANASI guys !!...            439   \n",
       "1  @noida_authority @CeoNoida Please Do permanent...              0   \n",
       "3  RT @STPIPune: @rsprasad @SanjayDhotreMP @Secre...             57   \n",
       "5  RT @_WeMeanToClean: Please retweet: Let's #Sto...              5   \n",
       "6  RT @chennaistpi: #SwachhtaPakhwada Pledge take...            136   \n",
       "\n",
       "   user_followers_count  favorite_count  place  coordinates  geo  \\\n",
       "0                   290               0    NaN          NaN  NaN   \n",
       "1                    28               0    NaN          NaN  NaN   \n",
       "3                   753               0    NaN          NaN  NaN   \n",
       "5                    45               0    NaN          NaN  NaN   \n",
       "6                   420               0    NaN          NaN  NaN   \n",
       "\n",
       "                       created_at               id_str  \n",
       "0  Sat Feb 08 06:52:10 +0000 2020  1226035980026535937  \n",
       "1  Sat Feb 08 06:51:51 +0000 2020  1226035898854199296  \n",
       "3  Sat Feb 08 06:51:02 +0000 2020  1226035693295525888  \n",
       "5  Sat Feb 08 06:50:33 +0000 2020  1226035572478750721  \n",
       "6  Sat Feb 08 06:50:25 +0000 2020  1226035539423326208  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.drop_duplicates(['full_text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RT @JayasreeVijayan: This is  VARANASI guys !!...</td>\n",
       "      <td>439</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:52:10 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@noida_authority @CeoNoida Please Do permanent...</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:51:51 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>RT @STPIPune: @rsprasad @SanjayDhotreMP @Secre...</td>\n",
       "      <td>57</td>\n",
       "      <td>753</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:51:02 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>RT @_WeMeanToClean: Please retweet: Let's #Sto...</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:50:33 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>RT @chennaistpi: #SwachhtaPakhwada Pledge take...</td>\n",
       "      <td>136</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:50:25 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>RT @chennaistpi: #STPICHENNAI organised worksh...</td>\n",
       "      <td>86</td>\n",
       "      <td>753</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:50:15 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>RT @chennaistpi: Cleanliness drive at STPI-Coi...</td>\n",
       "      <td>110</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:50:01 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>RT @chennaistpi: Cleanliness drive at STPI-Mad...</td>\n",
       "      <td>109</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:49:39 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>RT @smpurban: It's our duty to handover a clea...</td>\n",
       "      <td>2</td>\n",
       "      <td>9228</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:49:05 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Why you need to go #Switzerland? \\r\\n\\r\\n#Hima...</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:48:43 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Tarzan zinda hai? We thought we had lost him t...</td>\n",
       "      <td>0</td>\n",
       "      <td>6877</td>\n",
       "      <td>3</td>\n",
       "      <td>Sat Feb 08 06:45:58 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>RT @STPIHyderabad: Tree Plantation at STPI- Ti...</td>\n",
       "      <td>121</td>\n",
       "      <td>753</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:45:43 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>RT @STPIBengaluru: #Shramadhan by STPI-Mysuru ...</td>\n",
       "      <td>109</td>\n",
       "      <td>753</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:45:33 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>RT @STPIVizag: Landscaping and developing the ...</td>\n",
       "      <td>47</td>\n",
       "      <td>753</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:45:19 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>RT @STPIVizag: Cleaning/Beautification of gard...</td>\n",
       "      <td>55</td>\n",
       "      <td>753</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:45:15 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>RT @akshaykumar: Spotted : The newest #SwachhB...</td>\n",
       "      <td>4373</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:43:20 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>RT @knowthenation: \"Khoob achho kaam kiyo hai ...</td>\n",
       "      <td>188</td>\n",
       "      <td>284</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:42:10 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>RT @KolhapurStpi: #Swachhtapledge and awarenes...</td>\n",
       "      <td>56</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:38:57 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>RT @STPIVizag: Cleanliness drive carried out b...</td>\n",
       "      <td>47</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:36:15 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>RT @smpurban: Work in progress on Pratap Nagar...</td>\n",
       "      <td>91</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:35:46 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>RT @smpurban: Unsung heroes of Singrauli\\r\\nWh...</td>\n",
       "      <td>57</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:35:36 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>RT @smpurban: Learn waste management tips from...</td>\n",
       "      <td>77</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:33:06 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>RT @WesternRly: WR has painted one of its subu...</td>\n",
       "      <td>52</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:31:38 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>Let's segregate our waste with @_WeMeanToClean...</td>\n",
       "      <td>0</td>\n",
       "      <td>1172</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:30:25 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>Please retweet: Let's #StopLittering for #Clea...</td>\n",
       "      <td>5</td>\n",
       "      <td>2037</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:30:10 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>Four mini waste-to-compost plants to come up i...</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:30:10 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>RT @stpivjw1: Identification of e-waste is don...</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:27:30 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>RT @stpitvpm: Workshop on 'Waste management, P...</td>\n",
       "      <td>52</td>\n",
       "      <td>382</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:27:17 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>RT @smpurban: Switching from disposable Tea Ba...</td>\n",
       "      <td>120</td>\n",
       "      <td>784</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:27:00 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>RT @STPIGANDHINAGAR: #Shramdaan by STPI-Gandhi...</td>\n",
       "      <td>45</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:26:55 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>RT @Mumbaistpi: As part of #SwachhtaPakhwada t...</td>\n",
       "      <td>70</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:26:33 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>RT @STPIBengaluru: STPI-Mangaluru staff carrie...</td>\n",
       "      <td>148</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:26:30 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>RT @STPIGANDHINAGAR: Cleaning of Diesel Genera...</td>\n",
       "      <td>43</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:26:22 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>RT @Mumbaistpi: As part of #SwachhtaPakhwada W...</td>\n",
       "      <td>68</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Feb 08 06:26:19 +0000 2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            full_text  retweet_count  \\\n",
       "0   RT @JayasreeVijayan: This is  VARANASI guys !!...            439   \n",
       "1   @noida_authority @CeoNoida Please Do permanent...              0   \n",
       "3   RT @STPIPune: @rsprasad @SanjayDhotreMP @Secre...             57   \n",
       "5   RT @_WeMeanToClean: Please retweet: Let's #Sto...              5   \n",
       "6   RT @chennaistpi: #SwachhtaPakhwada Pledge take...            136   \n",
       "7   RT @chennaistpi: #STPICHENNAI organised worksh...             86   \n",
       "8   RT @chennaistpi: Cleanliness drive at STPI-Coi...            110   \n",
       "11  RT @chennaistpi: Cleanliness drive at STPI-Mad...            109   \n",
       "13  RT @smpurban: It's our duty to handover a clea...              2   \n",
       "14  Why you need to go #Switzerland? \\r\\n\\r\\n#Hima...              0   \n",
       "19  Tarzan zinda hai? We thought we had lost him t...              0   \n",
       "20  RT @STPIHyderabad: Tree Plantation at STPI- Ti...            121   \n",
       "21  RT @STPIBengaluru: #Shramadhan by STPI-Mysuru ...            109   \n",
       "22  RT @STPIVizag: Landscaping and developing the ...             47   \n",
       "23  RT @STPIVizag: Cleaning/Beautification of gard...             55   \n",
       "25  RT @akshaykumar: Spotted : The newest #SwachhB...           4373   \n",
       "28  RT @knowthenation: \"Khoob achho kaam kiyo hai ...            188   \n",
       "33  RT @KolhapurStpi: #Swachhtapledge and awarenes...             56   \n",
       "39  RT @STPIVizag: Cleanliness drive carried out b...             47   \n",
       "41  RT @smpurban: Work in progress on Pratap Nagar...             91   \n",
       "42  RT @smpurban: Unsung heroes of Singrauli\\r\\nWh...             57   \n",
       "46  RT @smpurban: Learn waste management tips from...             77   \n",
       "49  RT @WesternRly: WR has painted one of its subu...             52   \n",
       "51  Let's segregate our waste with @_WeMeanToClean...              0   \n",
       "55  Please retweet: Let's #StopLittering for #Clea...              5   \n",
       "56  Four mini waste-to-compost plants to come up i...              0   \n",
       "59  RT @stpivjw1: Identification of e-waste is don...             33   \n",
       "60  RT @stpitvpm: Workshop on 'Waste management, P...             52   \n",
       "61  RT @smpurban: Switching from disposable Tea Ba...            120   \n",
       "62  RT @STPIGANDHINAGAR: #Shramdaan by STPI-Gandhi...             45   \n",
       "63  RT @Mumbaistpi: As part of #SwachhtaPakhwada t...             70   \n",
       "64  RT @STPIBengaluru: STPI-Mangaluru staff carrie...            148   \n",
       "66  RT @STPIGANDHINAGAR: Cleaning of Diesel Genera...             43   \n",
       "67  RT @Mumbaistpi: As part of #SwachhtaPakhwada W...             68   \n",
       "\n",
       "    user_followers_count  favorite_count                      created_at  \n",
       "0                    290               0  Sat Feb 08 06:52:10 +0000 2020  \n",
       "1                     28               0  Sat Feb 08 06:51:51 +0000 2020  \n",
       "3                    753               0  Sat Feb 08 06:51:02 +0000 2020  \n",
       "5                     45               0  Sat Feb 08 06:50:33 +0000 2020  \n",
       "6                    420               0  Sat Feb 08 06:50:25 +0000 2020  \n",
       "7                    753               0  Sat Feb 08 06:50:15 +0000 2020  \n",
       "8                    420               0  Sat Feb 08 06:50:01 +0000 2020  \n",
       "11                   420               0  Sat Feb 08 06:49:39 +0000 2020  \n",
       "13                  9228               0  Sat Feb 08 06:49:05 +0000 2020  \n",
       "14                    73               0  Sat Feb 08 06:48:43 +0000 2020  \n",
       "19                  6877               3  Sat Feb 08 06:45:58 +0000 2020  \n",
       "20                   753               0  Sat Feb 08 06:45:43 +0000 2020  \n",
       "21                   753               0  Sat Feb 08 06:45:33 +0000 2020  \n",
       "22                   753               0  Sat Feb 08 06:45:19 +0000 2020  \n",
       "23                   753               0  Sat Feb 08 06:45:15 +0000 2020  \n",
       "25                     0               0  Sat Feb 08 06:43:20 +0000 2020  \n",
       "28                   284               0  Sat Feb 08 06:42:10 +0000 2020  \n",
       "33                   120               0  Sat Feb 08 06:38:57 +0000 2020  \n",
       "39                    34               0  Sat Feb 08 06:36:15 +0000 2020  \n",
       "41                   129               0  Sat Feb 08 06:35:46 +0000 2020  \n",
       "42                    99               0  Sat Feb 08 06:35:36 +0000 2020  \n",
       "46                     5               0  Sat Feb 08 06:33:06 +0000 2020  \n",
       "49                    53               0  Sat Feb 08 06:31:38 +0000 2020  \n",
       "51                  1172               0  Sat Feb 08 06:30:25 +0000 2020  \n",
       "55                  2037               0  Sat Feb 08 06:30:10 +0000 2020  \n",
       "56                   115               0  Sat Feb 08 06:30:10 +0000 2020  \n",
       "59                    34               0  Sat Feb 08 06:27:30 +0000 2020  \n",
       "60                   382               0  Sat Feb 08 06:27:17 +0000 2020  \n",
       "61                   784               0  Sat Feb 08 06:27:00 +0000 2020  \n",
       "62                    34               0  Sat Feb 08 06:26:55 +0000 2020  \n",
       "63                    34               0  Sat Feb 08 06:26:33 +0000 2020  \n",
       "64                    34               0  Sat Feb 08 06:26:30 +0000 2020  \n",
       "66                    34               0  Sat Feb 08 06:26:22 +0000 2020  \n",
       "67                    34               0  Sat Feb 08 06:26:19 +0000 2020  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.drop([\"place\",\"coordinates\",\"geo\",\"id_str\"],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "t=(len(df.index))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tag import pos_tag,map_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "pstem = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def clean_df(df_copy):\n",
    "    \n",
    "    #DROPS\n",
    "    #CHOOSE EITHER TO DROP ALL-ROW DUPLICATES OR FULL_TEXT DUPLICATES\n",
    "    df_copy=df_copy.drop_duplicates(['full_text']) #3377 left after this\n",
    "    df_copy=df_copy.reset_index(drop=True)\n",
    "    df_copy=df_copy.drop(['place','coordinates','geo','id_str'],axis=1)\n",
    "    #df_copy=\n",
    "    \n",
    "    # BASIC CLEANING FUNCTION\n",
    "    for i in range(len(df_copy)):\n",
    "        txt = df_copy.loc[i][\"full_text\"]\n",
    "        txt=re.sub(r'@[A-Z0-9a-z_:]+','',txt)#username-tags\n",
    "        txt=re.sub(r'^[RT]+','',txt)#RT-tags\n",
    "        txt = re.sub('https?://[A-Za-z0-9./]+','',txt)#URLs\n",
    "        txt=re.sub(\"[^a-zA-Z]\", \" \",txt)#hashtags\n",
    "        df_copy.at[i,\"full_text\"]=txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8f in position 2789: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-0205f966f129>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfieldnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"FirstName\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"LastName\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"IDNumber\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Message\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfieldnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjsonfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mjsonfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\csv.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mlf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8f in position 2789: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import csv, json\n",
    "csvfile = open('som2.csv', 'r')\n",
    "jsonfile = open('file.json', 'w')\n",
    "\n",
    "fieldnames = (\"FirstName\",\"LastName\",\"IDNumber\",\"Message\")\n",
    "reader = csv.DictReader( csvfile, fieldnames)\n",
    "for row in reader:\n",
    "    json.dump(row, jsonfile)\n",
    "    jsonfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "import re\n",
    "import csv\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class TweetCleaner:\n",
    "    def __init__(self, remove_stop_words=False, remove_retweets=False, stopwords_file='NLTK_DEFAULT'):\n",
    "        \"\"\"\n",
    "        clean unnecessary twitter data\n",
    "        remove_stop_words = True if stopwords are to be removed (default = False)\n",
    "        remove_retweets = True if retweets are to be removed (default = False)\n",
    "        stopwords_file = file containing stopwords(one on each line) (default: nltk english stopwords)\n",
    "        \"\"\"\n",
    "        if remove_stop_words:\n",
    "            if stopwords_file == 'NLTK_DEFAULT':\n",
    "                self.stop_words = set(stopwords.words('english'))\n",
    "            else:\n",
    "                stop_words = set()\n",
    "                with open(stopwords_file,'r') as f:\n",
    "                    for line in f:\n",
    "                        line = line.replace('\\n','')\n",
    "                        stop_words.add(line.lower())\n",
    "                        self.stop_words = stop_words\n",
    "        else:\n",
    "            self.stop_words = set()\n",
    "        \n",
    "        self.remove_retweets = remove_retweets\n",
    "        \n",
    "        self.punc_table = str.maketrans(\"\", \"\", string.punctuation) # to remove punctuation from each word in tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "    def compound_word_split(self, compound_word):\n",
    "        \"\"\"\n",
    "        Split a given compound word(string) and return list of words in given compound_word\n",
    "        Ex: compound_word='pyTWEETCleaner' --> ['py', 'TWEET', 'Cleaner']\n",
    "        \"\"\"\n",
    "        matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', compound_word)\n",
    "        return [m.group(0) for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def remove_non_ascii_chars(self, text):\n",
    "            \"\"\"\n",
    "            return text after removing non-ascii characters i.e. characters with ascii value >= 128\n",
    "            \"\"\"\n",
    "            return ''.join([w if ord(w) < 128 else ' ' for w in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def remove_hyperlinks(self,text):\n",
    "            \"\"\"\n",
    "            return text after removing hyperlinks\n",
    "            \"\"\"\n",
    "            return ' '.join([w for w in text.split(' ')  if not 'http' in w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def get_cleaned_text(self, text):\n",
    "            \"\"\"\n",
    "            return cleaned text(string) for provided tweet text(string)\n",
    "            \"\"\"\n",
    "            cleaned_text = text.replace('\\\"','').replace('\\'','').replace('-',' ')\n",
    "            cleaned_text =  self.remove_non_ascii_chars(cleaned_text)\n",
    "\n",
    "            # retweet\n",
    "            if re.match(r'RT @[_A-Za-z0-9]+:',cleaned_text): # retweet\n",
    "                if self.remove_retweets: return ''\n",
    "                retweet_info = cleaned_text[:cleaned_text.index(':')+2] # 'RT @name: ' will be again added in the text after cleaning\n",
    "                cleaned_text = cleaned_text[cleaned_text.index(':')+2:]\n",
    "            else:\n",
    "                retweet_info = ''\n",
    "            cleaned_text = self.remove_hyperlinks(cleaned_text)\n",
    "            cleaned_text = cleaned_text.replace('#','HASHTAGSYMBOL').replace('@','ATSYMBOL') # to avoid being removed while removing punctuations\n",
    "\n",
    "            tokens = [w.translate(self.punc_table) for w in word_tokenize(cleaned_text)] # remove punctuations and tokenize\n",
    "            tokens = [w for w in tokens if not w.lower() in self.stop_words and len(w)>1] # remove stopwords and single length words\n",
    "            cleaned_text = ' '.join(tokens)\n",
    "\n",
    "            cleaned_text = cleaned_text.replace('HASHTAGSYMBOL','#').replace('ATSYMBOL','@')\n",
    "            cleaned_text = retweet_info + cleaned_text\n",
    "\n",
    "            return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def get_cleaned_tweet(self, tweet):\n",
    "            \"\"\"\n",
    "            return a json dictionary of cleaned data from provided original tweet json dictionary\n",
    "            \"\"\"\n",
    "            if not \"created_at\" in tweet: return None # remove info about deleted tweets\n",
    "            if not tweet['lang'] == 'en': return None # remove tweets in non english language\n",
    "            if not tweet['in_reply_to_status_id'] == None or not tweet['in_reply_to_user_id'] == None: return None # remove comments of any tweet\n",
    "\n",
    "            cleaned_text = self.get_cleaned_text(tweet['text'])\n",
    "            if cleaned_text == '': return None\n",
    "\n",
    "            cleaned_tweet = {}\n",
    "\n",
    "            cleaned_tweet['created_at'] = tweet['created_at']\n",
    "            cleaned_tweet['full_text'] = cleaned_text\n",
    "\n",
    "            cleaned_tweet['user'] = {}\n",
    "            cleaned_tweet['user']['favourite_count'] = tweet['user']['favourite_count']\n",
    "\n",
    "\n",
    "            cleaned_tweet['retweet_count'] = tweet['retweet_count']\n",
    "\n",
    "            return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def clean_tweets(self, input_file, output_file='cleaned_tweets.json'):    \n",
    "            \"\"\"\n",
    "            input_file: name or path of input twitter json data where each line is a json tweet\n",
    "            output_file: file name or path where cleaned twitter json data is stored (default='cleaned_tweets.json')\n",
    "            \"\"\"\n",
    "            in_file = open(input_file, 'r')\n",
    "            out_file = open(output_file, 'w')\n",
    "\n",
    "            while True:\n",
    "                line = in_file.readline()\n",
    "                if line=='': break\n",
    "                tweet = json.loads(line)\n",
    "\n",
    "                cleaned_tweet = self.get_cleaned_tweet(tweet)\n",
    "                if cleaned_tweet == None: continue\n",
    "                \"\"\"\n",
    "                if 'retweeted_status' in tweet: # will be present if it is a retweet\n",
    "                    cleaned_tweet['retweeted_status'] = self.get_cleaned_tweet(tweet['retweeted_status'])\n",
    "                    if cleaned_tweet['retweeted_status'] == None: continue\n",
    "                \"\"\"\n",
    "\n",
    "                out_file.write(json.dumps(cleaned_tweet)+'\\n')\n",
    "\n",
    "\n",
    "            in_file.close()\n",
    "            out_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TweetCleaner' object has no attribute 'clean_tweets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-4cbab7bce19c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTweetCleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremove_stop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_retweets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'file.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cleaned_tweets.json'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# clean tweets from entire file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output with remove_stop_words=False, remove_retweets=False:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cleaned_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TweetCleaner' object has no attribute 'clean_tweets'"
     ]
    }
   ],
   "source": [
    "if __name__  == '__main__':\n",
    "    \n",
    "    tc = TweetCleaner(remove_stop_words=False, remove_retweets=False)\n",
    "    tc.clean_tweets(input_file='file.json', output_file='cleaned_tweets.json') # clean tweets from entire file\n",
    "    print('Output with remove_stop_words=False, remove_retweets=False:')\n",
    "    print(tc.get_cleaned_text(sample_text), '\\n')\n",
    "    \n",
    "    tc = TweetCleaner(remove_stop_words=False, remove_retweets=True)\n",
    "    print('Output with remove_stop_words=False, remove_retweets=True:')\n",
    "    print(tc.get_cleaned_text(sample_text), '\\n')\n",
    "    \n",
    "    tc = TweetCleaner(remove_stop_words=True, remove_retweets=False, stopwords_file='user_stopwords.txt')\n",
    "    print('Output with remove_stop_words=True, remove_retweets=False:')\n",
    "    print(tc.get_cleaned_text(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-e1663a061a9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"full_text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtxt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'@[A-Z0-9a-z_:]+'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#replace username-tags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtxt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'^[RT]+'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#replace RT-tags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for i in range(len(df)):\n",
    "    txt = df.loc[i][\"full_text\"]\n",
    "    txt=re.sub(r'@[A-Z0-9a-z_:]+','',txt)#replace username-tags\n",
    "    txt=re.sub(r'^[RT]+','',txt)#replace RT-tags\n",
    "    txt = re.sub('https?://[A-Za-z0-9./]+','',txt)#replace URLs\n",
    "    txt=re.sub(\"[^a-zA-Z]\", \" \",txt)#replace hashtags\n",
    "    df.at[i,\"full_text\"]=txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nif __name__  == \\'__main__\\':\\n    sample_text = \\'RT @testUser: Cleaning unnecessary data with pyTweetCleaner by @kevalMorabia97. #pyTWEETCleaner, Check it out at https:\\\\/\\\\/github.com\\\\/kevalmorabia97\\\\/pyTweetCleaner and star the repo!\\'\\n     \\n    tc = TweetCleaner(remove_stop_words=False, remove_retweets=False)\\n    tc.clean_tweets(input_file=\\'file.json\\', output_file=\\'cleaned_tweets.json\\') # clean tweets from entire file\\n    print(\\'Output with remove_stop_words=False, remove_retweets=False:\\')\\n    print(tc.get_cleaned_text(sample_text), \\'\\n\\')\\n    \\n    tc = TweetCleaner(remove_stop_words=False, remove_retweets=True)\\n    print(\\'Output with remove_stop_words=False, remove_retweets=True:\\')\\n    print(tc.get_cleaned_text(sample_text), \\'\\n\\')\\n    \\n    tc = TweetCleaner(remove_stop_words=True, remove_retweets=False, stopwords_file=\\'user_stopwords.txt\\')\\n    print(\\'Output with remove_stop_words=True, remove_retweets=False:\\')\\n    print(tc.get_cleaned_text(sample_text))\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import re\n",
    "import string \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "class TweetCleaner:\n",
    "    def __init__(self, remove_stop_words=False, remove_retweets=False, stopwords_file='NLTK_DEFAULT'):\n",
    "        \"\"\"\n",
    "        clean unnecessary twitter data\n",
    "        remove_stop_words = True if stopwords are to be removed (default = False)\n",
    "        remove_retweets = True if retweets are to be removed (default = False)\n",
    "        stopwords_file = file containing stopwords(one on each line) (default: nltk english stopwords)\n",
    "        \"\"\"\n",
    "        \n",
    "        if remove_stop_words:\n",
    "            if stopwords_file == 'NLTK_DEFAULT':\n",
    "                self.stop_words = set(stopwords.words('english'))\n",
    "            else:\n",
    "                stop_words = set()\n",
    "                with open(stopwords_file,'r') as f:\n",
    "                    for line in f:\n",
    "                        line = line.replace('\\n','')\n",
    "                        stop_words.add(line.lower())\n",
    "                    self.stop_words = stop_words\n",
    "        else:\n",
    "            self.stop_words = set()\n",
    "        \n",
    "        self.remove_retweets = remove_retweets\n",
    "        \n",
    "        self.punc_table = str.maketrans(\"\", \"\", string.punctuation) # to remove punctuation from each word in tokenize\n",
    "    \n",
    "    def compound_word_split(self, compound_word):\n",
    "        \"\"\"\n",
    "        Split a given compound word(string) and return list of words in given compound_word\n",
    "        Ex: compound_word='pyTWEETCleaner' --> ['py', 'TWEET', 'Cleaner']\n",
    "        \"\"\"\n",
    "        matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', compound_word)\n",
    "        return [m.group(0) for m in matches]\n",
    "    \n",
    "    def remove_non_ascii_chars(self, text):\n",
    "        \"\"\"\n",
    "        return text after removing non-ascii characters i.e. characters with ascii value >= 128\n",
    "        \"\"\"\n",
    "        return ''.join([w if ord(w) < 128 else ' ' for w in text])\n",
    "    \n",
    "    def remove_hyperlinks(self,text):\n",
    "        \"\"\"\n",
    "        return text after removing hyperlinks\n",
    "        \"\"\"\n",
    "        return ' '.join([w for w in text.split(' ')  if not 'http' in w])\n",
    "      \n",
    "    def get_cleaned_text(self, text):\n",
    "        \"\"\"\n",
    "        return cleaned text(string) for provided tweet text(string)\n",
    "        \"\"\"\n",
    "        cleaned_text = text.replace('\\\"','').replace('\\'','').replace('-',' ')\n",
    "            \n",
    "        cleaned_text =  self.remove_non_ascii_chars(cleaned_text)\n",
    "        \n",
    "        # retweet\n",
    "        if re.match(r'RT @[_A-Za-z0-9]+:',cleaned_text): # retweet\n",
    "            if self.remove_retweets: return ''\n",
    "            retweet_info = cleaned_text[:cleaned_text.index(':')+2] # 'RT @name: ' will be again added in the text after cleaning\n",
    "            cleaned_text = cleaned_text[cleaned_text.index(':')+2:]\n",
    "        else:\n",
    "            retweet_info = ''\n",
    "            \n",
    "        cleaned_text = self.remove_hyperlinks(cleaned_text)\n",
    "        \n",
    "        cleaned_text = cleaned_text.replace('#','HASHTAGSYMBOL').replace('@','ATSYMBOL') # to avoid being removed while removing punctuations\n",
    "        \n",
    "        tokens = [w.translate(self.punc_table) for w in word_tokenize(cleaned_text)] # remove punctuations and tokenize\n",
    "        tokens = [w for w in tokens if not w.lower() in self.stop_words and len(w)>1] # remove stopwords and single length words\n",
    "        cleaned_text = ' '.join(tokens)\n",
    "        \n",
    "        cleaned_text = cleaned_text.replace('HASHTAGSYMBOL','#').replace('ATSYMBOL','@')\n",
    "        cleaned_text = retweet_info + cleaned_text\n",
    "        \n",
    "        return cleaned_text\n",
    "        \n",
    "    def get_cleaned_tweet(self, tweet):\n",
    "        \"\"\"\n",
    "        return a json dictionary of cleaned data from provided original tweet json dictionary\n",
    "        \"\"\"\n",
    "        if not \"created_at\" in tweet: return None # remove info about deleted tweets\n",
    "        if not tweet['lang'] == 'en': return None # remove tweets in non english language\n",
    "        if not tweet['in_reply_to_status_id'] == None or not tweet['in_reply_to_user_id'] == None: return None # remove comments of any tweet\n",
    "        \n",
    "        cleaned_text = self.get_cleaned_text(tweet['text'])\n",
    "        if cleaned_text == '': return None\n",
    "\n",
    "        cleaned_tweet = {}\n",
    "        \n",
    "        cleaned_tweet['created_at'] = tweet['created_at']\n",
    "        cleaned_tweet['id'] = tweet['id']\n",
    "        cleaned_tweet['text'] = cleaned_text\n",
    "        \n",
    "        cleaned_tweet['user'] = {}\n",
    "        cleaned_tweet['user']['id'] = tweet['user']['id']\n",
    "        cleaned_tweet['user']['name'] = tweet['user']['name']\n",
    "        cleaned_tweet['user']['screen_name'] = tweet['user']['screen_name']\n",
    "        cleaned_tweet['user']['followers_count'] = tweet['user']['followers_count']\n",
    "         \n",
    "        cleaned_tweet['coordinates'] = tweet['coordinates']\n",
    "        cleaned_tweet['place'] = tweet['place']\n",
    "        cleaned_tweet['retweet_count'] = tweet['retweet_count']\n",
    "        cleaned_tweet['entities'] = tweet['entities']\n",
    "\n",
    "        return cleaned_tweet\n",
    "\n",
    "    def clean_tweets(self, input_file, output_file='cleaned_tweets.json'):    \n",
    "        \"\"\"\n",
    "        input_file: name or path of input twitter json data where each line is a json tweet\n",
    "        output_file: file name or path where cleaned twitter json data is stored (default='cleaned_tweets.json')\n",
    "        \"\"\"\n",
    "        in_file = open(input_file, 'r')\n",
    "        out_file = open(output_file, 'w')\n",
    "        \n",
    "        while True:\n",
    "            line = in_file.readline()\n",
    "            if line=='': break\n",
    "            tweet = json.loads(line)\n",
    "            \n",
    "            cleaned_tweet = self.get_cleaned_tweet(tweet)\n",
    "            if cleaned_tweet == None: continue\n",
    "            \n",
    "            if 'retweeted_status' in tweet: # will be present if it is a retweet\n",
    "                cleaned_tweet['retweeted_status'] = self.get_cleaned_tweet(tweet['retweeted_status'])\n",
    "                if cleaned_tweet['retweeted_status'] == None: continue\n",
    "                \n",
    "            out_file.write(json.dumps(cleaned_tweet)+'\\n')\n",
    "        \n",
    "        in_file.close()\n",
    "        out_file.close()\n",
    "\"\"\"\"\n",
    "if __name__  == '__main__':\n",
    "    sample_text = 'RT @testUser: Cleaning unnecessary data with pyTweetCleaner by @kevalMorabia97. #pyTWEETCleaner, Check it out at https:\\/\\/github.com\\/kevalmorabia97\\/pyTweetCleaner and star the repo!'\n",
    "     \n",
    "    tc = TweetCleaner(remove_stop_words=False, remove_retweets=False)\n",
    "    tc.clean_tweets(input_file='file.json', output_file='cleaned_tweets.json') # clean tweets from entire file\n",
    "    print('Output with remove_stop_words=False, remove_retweets=False:')\n",
    "    print(tc.get_cleaned_text(sample_text), '\\n')\n",
    "    \n",
    "    tc = TweetCleaner(remove_stop_words=False, remove_retweets=True)\n",
    "    print('Output with remove_stop_words=False, remove_retweets=True:')\n",
    "    print(tc.get_cleaned_text(sample_text), '\\n')\n",
    "    \n",
    "    tc = TweetCleaner(remove_stop_words=True, remove_retweets=False, stopwords_file='user_stopwords.txt')\n",
    "    print('Output with remove_stop_words=True, remove_retweets=False:')\n",
    "    print(tc.get_cleaned_text(sample_text))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 1 column 168 (char 167)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-ad85cabc54b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTweetCleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremove_stop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_retweets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'file.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cleaned_tweets.json'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# clean tweets from entire file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output with remove_stop_words=False, remove_retweets=False:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cleaned_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-c149beb08999>\u001b[0m in \u001b[0;36mclean_tweets\u001b[1;34m(self, input_file, output_file)\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m             \u001b[0mtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mcleaned_tweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cleaned_tweet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \"\"\"\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 1 column 168 (char 167)"
     ]
    }
   ],
   "source": [
    "if __name__  == '__main__':\n",
    "    sample_text = 'file.json'\n",
    "     \n",
    "    tc = TweetCleaner(remove_stop_words=False, remove_retweets=False)\n",
    "    tc.clean_tweets(input_file='file.json', output_file='cleaned_tweets.json') # clean tweets from entire file\n",
    "    print('Output with remove_stop_words=False, remove_retweets=False:')\n",
    "    print(tc.get_cleaned_text(sample_text), '\\n')\n",
    "    \n",
    "    tc = TweetCleaner(remove_stop_words=False, remove_retweets=True)\n",
    "    print('Output with remove_stop_words=False, remove_retweets=True:')\n",
    "    print(tc.get_cleaned_text(sample_text), '\\n')\n",
    "    \n",
    "    tc = TweetCleaner(remove_stop_words=True, remove_retweets=False, stopwords_file='user_stopwords.txt')\n",
    "    print('Output with remove_stop_words=True, remove_retweets=False:')\n",
    "    print(tc.get_cleaned_text(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
